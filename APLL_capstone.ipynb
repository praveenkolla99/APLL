{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## APLL CAPSTONE PROJECT"
      ],
      "metadata": {
        "id": "mgSUr36J3_HD"
      },
      "id": "mgSUr36J3_HD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: There are two modules in this file and both work independently, module 1 can be skipped if only the deep learning module is to be used since we do not need to score the data for ranking the tickets"
      ],
      "metadata": {
        "id": "ZXF2ywM84DUF"
      },
      "id": "ZXF2ywM84DUF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf77da6-caba-404b-b083-2a18b2a5fd15",
      "metadata": {
        "id": "1bf77da6-caba-404b-b083-2a18b2a5fd15",
        "outputId": "5e6c9c96-a279-46f4-fdad-00c0d2914830"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/marlanithin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/marlanithin/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/marlanithin/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importing all the libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential, model_from_json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing dataset and corpus"
      ],
      "metadata": {
        "id": "XxhYsnBsvoxc"
      },
      "id": "XxhYsnBsvoxc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edacf2f6-1906-47ee-9b70-dd175f59768b",
      "metadata": {
        "id": "edacf2f6-1906-47ee-9b70-dd175f59768b"
      },
      "outputs": [],
      "source": [
        "# import the dataset to be scored\n",
        "data = pd.read_csv('')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module-1 scoring the resolution notes using nlp libraries"
      ],
      "metadata": {
        "id": "5COCAptx33zG"
      },
      "id": "5COCAptx33zG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3709be01-8dfc-4146-850b-04a0b48f6095",
      "metadata": {
        "id": "3709be01-8dfc-4146-850b-04a0b48f6095"
      },
      "outputs": [],
      "source": [
        "# importing the corpus with all the word count data obtained from the original dataset\n",
        "corpus = pd.read_csv('count_data_idf_lemma.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ce7b41-3442-4fb6-8d89-39afd0c98265",
      "metadata": {
        "id": "83ce7b41-3442-4fb6-8d89-39afd0c98265"
      },
      "outputs": [],
      "source": [
        "# calculating the idf score of each word\n",
        "corpus['idf'] = corpus['count'].apply(lambda x:math.log(95698/x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cbcf18-5e80-46d7-be5a-b03eee85e086",
      "metadata": {
        "id": "44cbcf18-5e80-46d7-be5a-b03eee85e086",
        "outputId": "c3f7e575-3176-4008-e042-63e52b956fa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/2k/tg6w50fn00d15nxwt24cvmmc0000gn/T/ipykernel_48092/3677486083.py:1: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
            "  dc_dict = corpus.set_index('word').T.to_dict('list')\n"
          ]
        }
      ],
      "source": [
        "# creating a dictionary to look up for the words and scores in corpus\n",
        "dc_dict = corpus.set_index('word').T.to_dict('list')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74fa317-4ed6-49d9-b88e-74147a384f22",
      "metadata": {
        "id": "e74fa317-4ed6-49d9-b88e-74147a384f22",
        "outputId": "23c458f6-57ff-442c-dd9c-69a8c5b88776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/marlanithin/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Initiating lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for scoring each resolution note"
      ],
      "metadata": {
        "id": "Ee26W6K-vuCp"
      },
      "id": "Ee26W6K-vuCp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0c8276-c089-47f7-88d8-6515c57693d8",
      "metadata": {
        "id": "ee0c8276-c089-47f7-88d8-6515c57693d8",
        "outputId": "4e296fb1-4b87-4155-a704-6059edaf9f0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.555499669067901"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# defining the main funcition that will score the sentences\n",
        "def get_score(sentence):\n",
        "    sentence = word_tokenize(sentence)\n",
        "    # print(sentence)\n",
        "    temp = []\n",
        "    for i in sentence:\n",
        "        i = str(TextBlob(i).correct())\n",
        "        temp.append(lemmatizer.lemmatize(i, get_wordnet_pos(i)))\n",
        "        temp.append(i)\n",
        "    # print(temp)\n",
        "    if len(temp)==0:\n",
        "        return 0\n",
        "    if len(temp)<=3:\n",
        "        return 2\n",
        "    score = 0\n",
        "    for i in temp:\n",
        "        if i in dc_dict.keys():\n",
        "            score+=dc_dict[i][1]\n",
        "            # print(score)\n",
        "        else:\n",
        "            score+=1\n",
        "    score = (score/len(temp))+0.3*len(temp)\n",
        "    return score\n",
        "get_score('User Request')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a43aaa1-5cce-4f14-9af6-25cb1b57eece",
      "metadata": {
        "id": "5a43aaa1-5cce-4f14-9af6-25cb1b57eece"
      },
      "outputs": [],
      "source": [
        "# scoring all resolution notes in the dataset\n",
        "data['idf_score']=data['NotesTokenizedStemmed'].apply(lambda x: get_score2(str(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module-2 Deep learning module application"
      ],
      "metadata": {
        "id": "8rQNPUTj5ER_"
      },
      "id": "8rQNPUTj5ER_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the keras model"
      ],
      "metadata": {
        "id": "GCBVDK8AvyTU"
      },
      "id": "GCBVDK8AvyTU"
    },
    {
      "cell_type": "code",
      "source": [
        "# load pickle\n",
        "vectorizer = pickle.load(open(\"vector.pickel\", \"rb\"))\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "metadata": {
        "id": "kPiwNE1gv7Iu"
      },
      "id": "kPiwNE1gv7Iu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data[['Resolution notes']]"
      ],
      "metadata": {
        "id": "c4YG8JwOv8fX"
      },
      "id": "c4YG8JwOv8fX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_corpus = [] # store all notes here so that we can fit the vectorizer with this dataset\n",
        "y = []\n",
        "for index, row in df.iterrows():\n",
        "  review_text = row['Resolution notes'] \n",
        "  review_corpus.append(str(review_text))\n",
        "\n",
        "vectorized_review = vectorizer.transform(review_corpus)\n",
        "\n",
        "X = vectorized_review.toarray()\n",
        "# predicting the satisfaction of resolution note using the loaded model\n",
        "pred = loaded_model.predict(X)"
      ],
      "metadata": {
        "id": "ClFDrj5uwOks"
      },
      "id": "ClFDrj5uwOks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining the predictions to the data table\n",
        "pred = pd.DataFrame(pred, columns=['pred'])\n",
        "result = pd.concat([data, pred], axis=1)"
      ],
      "metadata": {
        "id": "0SrhRvwIwplV"
      },
      "id": "0SrhRvwIwplV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['Root cause 1'].fillna('', inplace=True)\n",
        "result['Root cause 2'].fillna('', inplace=True)\n",
        "result['pred'] = result['pred'].apply(lambda x: 1 if x>0.5 else 0)"
      ],
      "metadata": {
        "id": "-WcXF5bFwt-S"
      },
      "id": "-WcXF5bFwt-S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ranking the data based on the prediction and root causes\n",
        "rank = []\n",
        "for i, j ,l in zip(result['pred'],result['Root cause 1'],result['Root cause 2']):\n",
        "  sum = 0\n",
        "  if i==1:\n",
        "    sum+=3\n",
        "  if j:\n",
        "    sum+=2\n",
        "  if l:\n",
        "    sum+=1\n",
        "  if sum==6:\n",
        "    rank.append(1)\n",
        "  elif sum==4 or sum==5:\n",
        "    rank.append(2)\n",
        "  elif sum==3:\n",
        "    rank.append(3)\n",
        "  elif sum==1 or sum==2:\n",
        "    rank.append(4)\n",
        "  else:\n",
        "    rank.append(5)"
      ],
      "metadata": {
        "id": "Mdz9gXrSwzBM"
      },
      "id": "Mdz9gXrSwzBM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = pd.DataFrame(rank, columns=['rank'])\n",
        "result = pd.concat([result, rank], axis=1)"
      ],
      "metadata": {
        "id": "itdyTyiww5vL"
      },
      "id": "itdyTyiww5vL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the file\n",
        "result.to_csv('ranked_data.csv',index=False)"
      ],
      "metadata": {
        "id": "CkK5YX0pxBAx"
      },
      "id": "CkK5YX0pxBAx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}